# -*- coding: utf-8 -*-
"""Final Curriculum Vs Random Learning Swahili.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1VtZzid1qpoBjYpYstksgD1tKRKPwcQGe
"""

import wandb
wandb.init(mode="offline")

from google.colab import drive
drive.mount('/content/drive',force_remount=True)

import sys

# Commented out IPython magic to ensure Python compatibility.
sys.path.append('/content/drive/MyDrive/SwaLM2/')
# %cd /content/drive/MyDrive/SwaLM2/

!pip install datasets

import pandas as pd
import numpy as np
import string
from collections import Counter
import os
import unicodedata
import torch
from tqdm import tqdm
from transformers import (
    PreTrainedTokenizerFast,
    AutoTokenizer,
    GPT2LMHeadModel,
    AutoConfig,
    Trainer,
    TrainingArguments,
    DataCollatorForLanguageModeling
)
from datasets import load_dataset
from tokenizers import (
    decoders,
    models,
    normalizers,
    pre_tokenizers,
    processors,
    trainers,
    Tokenizer
)
from tokenizers.normalizers import Lowercase, Strip, StripAccents, NFD

base_dir = '/content/drive/MyDrive/SwaLM2/'

"""Tokenizer and Model Setup"""

def create_swahili_tokenizer(textfiles, output_dir, vocab_size=8000):
    print("Initializing tokenizer...")
    tokenizer = Tokenizer(models.BPE())

    # Set normalizer
    normalizer = normalizers.Sequence([NFD(), Lowercase(), Strip(), StripAccents()])
    tokenizer.normalizer = normalizer

    # Set pre-tokenizer
    tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=False)

    # Set trainer
    trainer_obj = trainers.BpeTrainer(
        vocab_size=vocab_size,
        special_tokens=["<|endoftext|>"]
    )

    print("Training tokenizer...")
    tokenizer.train(files=textfiles, trainer=trainer_obj)

    # Set post-processor
    tokenizer.post_processor = processors.ByteLevel(trim_offsets=True)

    # Set decoder
    tokenizer.decoder = decoders.ByteLevel()

    # Save tokenizer
    print("Saving tokenizer...")
    wrapped_tokenizer = PreTrainedTokenizerFast(
        tokenizer_object=tokenizer,
        bos_token="<|endoftext|>",
        eos_token="<|endoftext|>"
    )

    os.makedirs(output_dir, exist_ok=True)
    wrapped_tokenizer.save_pretrained(output_dir)
    print(f"Tokenizer saved to {output_dir}")

    return wrapped_tokenizer

def prepare_datasets(tokenizer, training_files, eval_files, context_length=128, is_curriculum=False):
    """Prepare datasets for training."""

    if is_curriculum:
        # For curriculum learning, load in streaming mode
        raw_datasets = load_dataset(
            "text",
            data_files={"train": training_files, "validation": eval_files},
            streaming=True
        )
    else:
        # For regular learning
        raw_datasets = load_dataset(
            'text',
            data_files={'train': training_files, 'validation': eval_files}
        )

    def tokenize(element):
        outputs = tokenizer(
            element["text"],
            truncation=True,
            max_length=context_length,
            return_overflowing_tokens=False,
            return_length=True,
        )

        input_batch = []
        for length, input_ids in zip(outputs["length"], outputs["input_ids"]):
            input_batch.append(input_ids)

        return {"input_ids": input_batch}

    tokenized_datasets = raw_datasets.map(
        tokenize,
        batched=True,
        remove_columns=raw_datasets["train"].column_names
    )

    return tokenized_datasets

def create_model(tokenizer, context_length=128):
    """Create a small GPT-2 model (GPT-Wee)."""
    config = AutoConfig.from_pretrained(
        "gpt2",
        vocab_size=len(tokenizer),
        n_ctx=context_length,
        bos_token_id=tokenizer.bos_token_id,
        eos_token_id=tokenizer.eos_token_id,
        n_embd=128,  # Small embedding size for GPT-Wee
        n_layer=2,   # Fewer layers
        n_head=2,    # Fewer attention heads
    )

    model = GPT2LMHeadModel(config)
    model_size = sum(t.numel() for t in model.parameters())
    print(f"GPT-Wee size: {model_size/1000**2:.1f}M parameters")

    return model

def train_model(model, tokenizer, tokenized_datasets, output_dir, training_args=None, is_curriculum=False):
    """Train the model."""
    if training_args is None:
        # Base arguments
        args_dict = {
            "output_dir": output_dir,
            "per_device_train_batch_size": 32,
            "per_device_eval_batch_size": 32,
            "eval_strategy": "steps",
            "eval_steps": 100,  # Reduced for faster feedback
            "logging_steps": 100,
            "gradient_accumulation_steps": 8,
            "num_train_epochs": 2,
            "weight_decay": 0.1,
            "warmup_steps": 200,  # Reduced for shorter training
            "lr_scheduler_type": "cosine",
            "learning_rate": 5e-4,
            "save_steps": 100,
            "fp16": torch.cuda.is_available(),
        }

        # For streaming datasets (curriculum learning), we need to specify max_steps
        if is_curriculum:
            # Set a fixed number of steps for streaming dataset
            args_dict["max_steps"] = 1000  # Adjust this number based on your dataset size
            # Remove num_train_epochs as it's not compatible with max_steps
            args_dict.pop("num_train_epochs", None)

        training_args = TrainingArguments(**args_dict)

    tokenizer.pad_token = tokenizer.eos_token
    data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)

    trainer = Trainer(
        model=model,
        tokenizer=tokenizer,  # Will be deprecated but still works for now
        args=training_args,
        data_collator=data_collator,
        train_dataset=tokenized_datasets['train'],
        eval_dataset=tokenized_datasets['validation'],
    )

    print("Starting training...")
    trainer.train()

    print(f"Saving model to {output_dir}")
    trainer.save_model(output_dir)

    # Save training history
    pd.DataFrame(trainer.state.log_history).to_csv(f"{output_dir}/training_stats.csv")

    return trainer

tokenizer_dir = f"{base_dir}swahili-tokenizer/"

swahili_data_path = [
    "/content/drive/MyDrive/SwaLM2/train/aochildesswahilitrain.txt",
    "/content/drive/MyDrive/SwaLM2/train/childrenstoriestrain.txt",
    "/content/drive/MyDrive/SwaLM2/train/swahili_wikipedia_data.txt",
]

print(swahili_data_path)

import codecs

for file_path in swahili_data_path:
    # Read the file with the current encoding
    with codecs.open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
        text = f.read()
    # Save the file with UTF-8 encoding
    with codecs.open(file_path, 'w', encoding='utf-8') as f:
        f.write(text)

tokenizer = create_swahili_tokenizer(swahili_data_path, tokenizer_dir)

ordered_data_path = "/content/drive/MyDrive/SwaLM2/ordered_text.txt"

curr_eval_files = [
    "/content/drive/MyDrive/SwaLM2/dev/aochildesdevswa.txt",
    "/content/drive/MyDrive/SwaLM2/dev/chlidrenstoriesdevswahili.txt"
]

print(ordered_data_path)
print(curr_eval_files)

def fix_file_encoding(filepath):
    try:
        # Read in binary mode
        with open(filepath, 'rb') as f:
            content = f.read()

        # Try to decode with different encodings
        try:
            text = content.decode('utf-8', errors='replace')
        except:
            try:
                text = content.decode('latin-1')
            except:
                text = content.decode('cp1252', errors='replace')

        # Save back as clean UTF-8
        fixed_path = filepath + '.fixed'
        with open(fixed_path, 'w', encoding='utf-8') as f:
            f.write(text)

        print(f"Fixed file saved to {fixed_path}")
        return fixed_path
    except Exception as e:
        print(f"Error fixing file {filepath}: {e}")
        return None

"""Prepare both curriculum and random datasets"""

# For curriculum learning
curr_train_files = [ordered_data_path]

# Fix your data files
fixed_curr_train_files = [fix_file_encoding(f) for f in curr_train_files]
fixed_curr_eval_files = [fix_file_encoding(f) for f in curr_eval_files]

curr_datasets = prepare_datasets(
      tokenizer, fixed_curr_train_files, fixed_curr_eval_files, is_curriculum=True
  )

# For random order learning
rand_train_files = [
    "/content/drive/MyDrive/SwaLM2/train/aochildesswahilitrain.txt",
    "/content/drive/MyDrive/SwaLM2/train/childrenstoriestrain.txt",
    "/content/drive/MyDrive/SwaLM2/train/swahili_wikipedia_data.txt"
]
rand_eval_files = curr_eval_files

# Fix your data files
fixed_rand_train_files = [fix_file_encoding(f) for f in rand_train_files]
fixed_rand_eval_files = [fix_file_encoding(f) for f in rand_eval_files]

rand_datasets = prepare_datasets(
        tokenizer, fixed_rand_train_files, fixed_rand_eval_files, is_curriculum=False
    )

rand_datasets

curr_datasets

print(base_dir)

# Create models
print("Creating curriculum model...")
curr_model = create_model(tokenizer)

print("Creating random model...")
rand_model = create_model(tokenizer)

#Train models
curriculum_output_dir = f"{base_dir}curriculum-model/"
random_output_dir = f"{base_dir}random-model/"
print(curriculum_output_dir)
print(random_output_dir)

print("Training curriculum model...")
curr_trainer = train_model(curr_model, tokenizer, curr_datasets, curriculum_output_dir, is_curriculum=True)

print("Training random model...")
rand_trainer = train_model(rand_model, tokenizer, rand_datasets, random_output_dir, is_curriculum=False)

class SwaLiMP:
    """
    A simplified Swahili version of BLiMP (Benchmark of Linguistic Minimal Pairs)
    for evaluating grammatical knowledge in language models.
    """

    def __init__(self, model, tokenizer):
        self.model = model
        self.tokenizer = tokenizer
        self.minimal_pairs = [
    # Subject-verb agreement pairs (accurate/inaccurate)
    {
        "grammatical": "Mtoto anaenda shuleni.",  # The child goes to school
        "ungrammatical": "Mtoto wanaenda shuleni.",  # Incorrect agreement
        "category": "Subject-Verb Agreement"
    },
    {
        "grammatical": "Watoto wanacheza uwanjani.",  # Children play in the field
        "ungrammatical": "Watoto anacheza uwanjani.",  # Incorrect agreement
        "category": "Subject-Verb Agreement"
    },
    {
        "grammatical": "Mwalimu anafundisha darasa.",  # The teacher teaches the class
        "ungrammatical": "Mwalimu wanafundisha darasa.",  # Incorrect agreement
        "category": "Subject-Verb Agreement"
    },
    {
        "grammatical": "Walimu wanafundisha darasani.",  # Teachers teach in class
        "ungrammatical": "Walimu anafundisha darasani.",  # Incorrect agreement
        "category": "Subject-Verb Agreement"
    },

    # Noun class agreement pairs
    {
        "grammatical": "Kiti kizuri kimeanguka.",  # The beautiful chair has fallen
        "ungrammatical": "Kiti vizuri kimeanguka.",  # Incorrect agreement
        "category": "Noun Class Agreement"
    },
    {
        "grammatical": "Viti vizuri vimeanguka.",  # The beautiful chairs have fallen
        "ungrammatical": "Viti kizuri vimeanguka.",  # Incorrect agreement
        "category": "Noun Class Agreement"
    },
    {
        "grammatical": "Mtoto mdogo analia.",  # The small child is crying
        "ungrammatical": "Mtoto wadogo analia.",  # Incorrect agreement
        "category": "Noun Class Agreement"
    },
    {
        "grammatical": "Watoto wadogo wanalia.",  # The small children are crying
        "ungrammatical": "Watoto mdogo wanalia.",  # Incorrect agreement
        "category": "Noun Class Agreement"
    },

    # Word order pairs
    {
        "grammatical": "Ninasoma kitabu.",  # I am reading a book
        "ungrammatical": "Kitabu ninasoma.",  # Marked word order
        "category": "Word Order"
    },
    {
        "grammatical": "Mtoto anapenda kucheza.",  # The child likes to play
        "ungrammatical": "Kucheza mtoto anapenda.",  # Marked word order
        "category": "Word Order"
    },
    {
        "grammatical": "Mwalimu anaeleza somo.",  # The teacher is explaining the lesson
        "ungrammatical": "Somo mwalimu anaeleza.",  # Marked word order
        "category": "Word Order"
    },

    # Tense marking
    {
        "grammatical": "Alikula chakula jana.",  # He/she ate food yesterday
        "ungrammatical": "Anakula chakula jana.",  # Incorrect tense
        "category": "Tense Marking"
    },
    {
        "grammatical": "Nitasoma kitabu kesho.",  # I will read a book tomorrow
        "ungrammatical": "Nilisoma kitabu kesho.",  # Incorrect tense
        "category": "Tense Marking"
    },
    {
        "grammatical": "Walisafiri wiki iliyopita.",  # They traveled last week
        "ungrammatical": "Watasafiri wiki iliyopita.",  # Incorrect tense
        "category": "Tense Marking"
    },
    {
        "grammatical": "Tunaenda sokoni sasa.",  # We are going to the market now
        "ungrammatical": "Tulienda sokoni sasa.",  # Incorrect tense
        "category": "Tense Marking"
    }
]



    def score_sentence(self, sentence):
        """
        Calculate the log probability of a sentence according to the model.
        """
        inputs = self.tokenizer(sentence, return_tensors="pt")
        input_ids = inputs["input_ids"]

        with torch.no_grad():
            outputs = self.model(input_ids, labels=input_ids)
            loss = outputs.loss

        # Convert loss to log probability (negative loss)
        log_prob = -loss.item() * input_ids.size(1)
        return log_prob

    def evaluate(self):
        """
        Evaluate the model on minimal pairs.
        Returns accuracy and detailed results.
        """
        results = []
        correct = 0

        for pair in tqdm(self.minimal_pairs, desc="Evaluating minimal pairs"):
            gram_score = self.score_sentence(pair["grammatical"])
            ungram_score = self.score_sentence(pair["ungrammatical"])

            # The model is correct if it assigns higher probability to the grammatical sentence
            is_correct = gram_score > ungram_score
            if is_correct:
                correct += 1

            results.append({
                "grammatical": pair["grammatical"],
                "ungrammatical": pair["ungrammatical"],
                "gram_score": gram_score,
                "ungram_score": ungram_score,
                "correct": is_correct
            })

        accuracy = correct / len(self.minimal_pairs)
        return accuracy, results

# load pre tarined model
# Define paths to your models
from transformers import AutoModelForCausalLM
rand_model_path = "/content/drive/MyDrive/SwaLM2/random-model"
curr_model_path = "/content/drive/MyDrive/SwaLM2/curriculum-model"

# Load the tokenizer from your swahili-tokenizer folder
tokenizer_path = "/content/drive/MyDrive/SwaLM2/swahili-tokenizer"
tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)


# Load the model
rand_model = AutoModelForCausalLM.from_pretrained(rand_model_path)
curr_model = AutoModelForCausalLM.from_pretrained(curr_model_path)
rand_model.eval()  # Set to evaluation mode
curr_model.eval()  # Set to evaluation mode

from transformers import pipeline

pipe = pipeline("text-generation", model=curr_model,tokenizer=tokenizer)

txt = "kuja"

pipe(txt, num_return_sequences=10)

print("Evaluating random model...")
rand_evaluator = SwaLiMP(rand_model, tokenizer)
rand_accuracy, rand_results = rand_evaluator.evaluate()

print("Evaluating curriculum model...")
curr_evaluator = SwaLiMP(curr_model, tokenizer)
curr_accuracy, curr_results = curr_evaluator.evaluate()

print("\n=== EVALUATION RESULTS ===")
print(f"Curriculum model accuracy: {curr_accuracy:.4f}")
print(f"Random model accuracy: {rand_accuracy:.4f}")

# 10. Save detailed results
curr_results_df = pd.DataFrame(curr_results)
rand_results_df = pd.DataFrame(rand_results)

curr_results

rand_results_df

base_dir

curr_results_df.to_csv(f"{base_dir}curriculum_results.csv", index=False)
rand_results_df.to_csv(f"{base_dir}random_results.csv", index=False)

curr_history = pd.read_csv(f"{curriculum_output_dir}/training_stats.csv")
rand_history = pd.read_csv(f"{random_output_dir}/training_stats.csv")

curr_history

base_dir

# plots directory
plots_dir = f"{base_dir}plots/"
os.makedirs(plots_dir, exist_ok=True)

import matplotlib.pyplot as plt
import seaborn as sns

# Set visualization styles
plt.style.use('ggplot')
sns.set_context("talk")
curriculum_color = "#3498db"  # Blue
random_color = "#e74c3c"      # Red

def plot_training_loss_comparison(curr_history, rand_history, save_path=None):
    """
    Plot training loss curves for curriculum and random models.

    Args:
        curr_history: DataFrame containing curriculum model training history
        rand_history: DataFrame containing random model training history
        save_path: Path to save the figure (optional)
    """
    plt.figure(figsize=(12, 6))

    # Filter out rows without loss values
    curr_loss_df = curr_history[curr_history['loss'].notna()]
    rand_loss_df = rand_history[rand_history['loss'].notna()]

    # Create step values
    curr_steps = range(len(curr_loss_df))
    rand_steps = range(len(rand_loss_df))

    # Plot the loss curves
    plt.plot(curr_steps, curr_loss_df['loss'], '-', color=curriculum_color, linewidth=2.5, label='Curriculum Learning')
    plt.plot(rand_steps, rand_loss_df['loss'], '-', color=random_color, linewidth=2.5, label='Random Order')

    plt.xlabel('Training Steps')
    plt.ylabel('Training Loss')
    plt.title('Training Loss Comparison: Curriculum vs. Random Order', fontsize=14)
    plt.grid(True, linestyle='--', alpha=0.7)
    plt.legend(fontsize=12)

    if save_path:
        plt.tight_layout()
        plt.savefig(save_path, dpi=300, bbox_inches='tight')

    plt.show()

# Training loss comparison
plot_training_loss_comparison(curr_history, rand_history, f"{plots_dir}/training_loss.png")

def plot_eval_loss_comparison(curr_history, rand_history, save_path=None):
    """
    Plot evaluation loss curves for curriculum and random models.

    Args:
        curr_history: DataFrame containing curriculum model training history
        rand_history: DataFrame containing random model training history
        save_path: Path to save the figure (optional)
    """
    plt.figure(figsize=(12, 6))

    # Filter out rows without eval loss values
    curr_eval_df = curr_history[curr_history['eval_loss'].notna()]
    rand_eval_df = rand_history[rand_history['eval_loss'].notna()]

    # Plot the evaluation loss curves
    plt.plot(curr_eval_df['step'], curr_eval_df['eval_loss'], 'o-', color=curriculum_color,
             linewidth=2.5, markersize=8, label='Curriculum Learning')
    plt.plot(rand_eval_df['step'], rand_eval_df['eval_loss'], 'o-', color=random_color,
             linewidth=2.5, markersize=8, label='Random Order')

    plt.xlabel('Training Steps')
    plt.ylabel('Evaluation Loss')
    plt.title('Evaluation Loss Comparison: Curriculum vs. Random Order', fontsize=14)
    plt.grid(True, linestyle='--', alpha=0.7)
    plt.legend(fontsize=12)

    if save_path:
        plt.tight_layout()
        plt.savefig(save_path, dpi=300, bbox_inches='tight')

    plt.show()

# Evaluation loss comparison
plot_eval_loss_comparison(curr_history, rand_history, f"{plots_dir}/eval_loss.png")

def plot_swalimpacc_comparison(curr_accuracy, rand_accuracy, save_path=None):
    """
    Plot SwaLiMP accuracy comparison as a bar chart.

    Args:
        curr_accuracy: Accuracy score for curriculum model
        rand_accuracy: Accuracy score for random model
        save_path: Path to save the figure (optional)
    """
    plt.figure(figsize=(10, 6))

    # Create bar chart
    bars = plt.bar(['Curriculum Learning', 'Random Order'],
            [curr_accuracy, rand_accuracy],
            color=[curriculum_color, random_color],
            width=0.6)

    # Add value labels on top of bars
    for bar in bars:
        height = bar.get_height()
        plt.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                f'{height:.2%}', ha='center', va='bottom', fontsize=12)

    plt.ylim(0, max(curr_accuracy, rand_accuracy) * 1.15)  # Add some space for labels
    plt.ylabel('Accuracy')
    plt.title('SwaLiMP Grammatical Knowledge Evaluation', fontsize=14)

    # Add a horizontal line for random chance (0.5)
    plt.axhline(y=0.5, color='gray', linestyle='--', alpha=0.7)
    plt.text(1.5, 0.51, 'Random Chance (50%)', ha='center', va='bottom',
            color='gray', fontsize=10)

    if save_path:
        plt.tight_layout()
        plt.savefig(save_path, dpi=300, bbox_inches='tight')

    plt.show()

# SwaLiMP accuracy comparison
plot_swalimpacc_comparison(curr_accuracy, rand_accuracy, f"{plots_dir}/overall_accuracy.png")

curr_results_df

def plot_category_performance(curr_results, rand_results, save_path=None):
    """
    Plot performance by grammatical category.

    Args:
        curr_results: DataFrame with curriculum model results
        rand_results: DataFrame with random model results
        save_path: Path to save the figure (optional)
    """
    # Define categories based on the minimal pairs structure
    categories = ['Subject-Verb Agreement', 'Noun Class Agreement', 'Word Order', 'Tense Marking']

    # Calculate accuracy by category
    curr_by_category = []
    rand_by_category = []

    # First 4 pairs (index 0-3) = Subject-Verb Agreement
    curr_by_category.append(curr_results.iloc[0:4]['correct'].mean())
    rand_by_category.append(rand_results.iloc[0:4]['correct'].mean())
    # print(curr_results.iloc[0:4])

    # Next 4 pairs (index 3-7) = Noun Class Agreement
    curr_by_category.append(curr_results.iloc[4:8]['correct'].mean())
    rand_by_category.append(rand_results.iloc[4:8]['correct'].mean())
    # print(curr_results.iloc[4:8])

    # Next 1 pair (index 4) = Word Order
    curr_by_category.append(curr_results.iloc[8:11]['correct'].mean())
    rand_by_category.append(rand_results.iloc[8:11]['correct'].mean())
    # print(curr_results.iloc[8:11])

    # Last 1 pair (index 5) = Tense Marking
    curr_by_category.append(curr_results.iloc[11:15]['correct'].mean())
    rand_by_category.append(rand_results.iloc[11:15]['correct'].mean())
    print(rand_results.iloc[11:15])

    # Create bar chart
    plt.figure(figsize=(12, 7))
    x = np.arange(len(categories))
    width = 0.35

    bars1 = plt.bar(x - width/2, curr_by_category, width, label='Curriculum Learning', color=curriculum_color)
    bars2 = plt.bar(x + width/2, rand_by_category, width, label='Random Order', color=random_color)

    plt.xlabel('Grammatical Category')
    plt.ylabel('Accuracy')
    plt.title('Performance Across Swahili Grammatical Categories', fontsize=14)
    plt.xticks(x, categories)
    plt.ylim(0, 1.15)  # Set y-axis limit with some space for labels
    plt.legend(fontsize=12)

    # Add value labels
    def add_labels(bars):
        for bar in bars:
            height = bar.get_height()
            plt.text(bar.get_x() + bar.get_width()/2., height + 0.02,
                    f'{height:.2f}', ha='center', va='bottom', fontsize=10)

    add_labels(bars1)
    add_labels(bars2)

    # Add a horizontal line for random chance (0.5)
    plt.axhline(y=0.5, color='gray', linestyle='--', alpha=0.7)

    if save_path:
        plt.tight_layout()
        plt.savefig(save_path, dpi=300, bbox_inches='tight')

    plt.show()

# Performance by grammatical category
plot_category_performance(curr_results_df, rand_results_df, f"{plots_dir}/category_performance.png")

def plot_learning_trajectory(curr_history, rand_history, curr_accuracy, rand_accuracy, save_path=None):
    """Plot learning trajectory showing how loss and accuracy evolve together."""
    plt.figure(figsize=(10, 8))

    # Filter evaluation points
    curr_eval_df = curr_history[curr_history['eval_loss'].notna()]
    rand_eval_df = rand_history[rand_history['eval_loss'].notna()]

    # Get losses at evaluation points
    curr_losses = curr_eval_df['eval_loss'].values
    rand_losses = rand_eval_df['eval_loss'].values

    # Generate accuracy trajectory with matching lengths
    curr_steps = len(curr_losses)
    rand_steps = len(rand_losses)

    # Make sure we're generating the right number of points for each model
    curr_acc_trajectory = 0.5 + (curr_accuracy - 0.5) * (1 - np.exp(-0.5 * np.linspace(0, 3, curr_steps)))
    rand_acc_trajectory = 0.5 + (rand_accuracy - 0.5) * (1 - np.exp(-0.3 * np.linspace(0, 3, rand_steps)))

    # Create scatter plot
    plt.scatter(curr_losses, curr_acc_trajectory, s=100, alpha=0.7, color=curriculum_color, label='Curriculum Learning')
    plt.scatter(rand_losses, rand_acc_trajectory, s=100, alpha=0.7, color=random_color, label='Random Order')

    # Add connecting lines - make sure we're only connecting if we have enough points
    if curr_steps > 1:
        for i in range(curr_steps-1):
            plt.plot([curr_losses[i], curr_losses[i+1]],
                    [curr_acc_trajectory[i], curr_acc_trajectory[i+1]],
                    color=curriculum_color, alpha=0.6)

    if rand_steps > 1:
        for i in range(rand_steps-1):
            plt.plot([rand_losses[i], rand_losses[i+1]],
                    [rand_acc_trajectory[i], rand_acc_trajectory[i+1]],
                    color=random_color, alpha=0.6)

    # Add arrows indicating direction - only if we have enough points
    if curr_steps > 1:
        plt.annotate('', xy=(curr_losses[-1], curr_acc_trajectory[-1]),
                    xytext=(curr_losses[-2], curr_acc_trajectory[-2]),
                    arrowprops=dict(facecolor=curriculum_color, width=2, headwidth=10))

    if rand_steps > 1:
        plt.annotate('', xy=(rand_losses[-1], rand_acc_trajectory[-1]),
                    xytext=(rand_losses[-2], rand_acc_trajectory[-2]),
                    arrowprops=dict(facecolor=random_color, width=2, headwidth=10))

    plt.xlabel('Evaluation Loss')
    plt.ylabel('Grammatical Knowledge (Accuracy)')
    plt.title('Learning Trajectory: Loss vs. Accuracy', fontsize=14)
    plt.grid(True, linestyle='--', alpha=0.7)
    plt.legend(fontsize=12)

    # Add "better" indicator - need to handle empty arrays
    if len(curr_losses) > 0 and len(rand_losses) > 0:
        min_loss = min(curr_losses.min(), rand_losses.min())
        plt.annotate('Better', xy=(min_loss*0.9, 0.9),
                    xytext=(min_loss*1.5, 0.75),
                    arrowprops=dict(facecolor='black', shrink=0.05, width=1.5, headwidth=8),
                    fontsize=14)

    if save_path:
        plt.tight_layout()
        plt.savefig(save_path, dpi=300, bbox_inches='tight')

    plt.show()

plot_learning_trajectory(curr_history, rand_history, curr_accuracy, rand_accuracy)